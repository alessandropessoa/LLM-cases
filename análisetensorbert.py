# -*- coding: utf-8 -*-
"""AnáliseTensorBert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uIn6i3erJY5zDZ9Xzyrqnljbbqg2M8oD

# Importação de pacotes
"""

from transformers import BertTokenizer, BertModel
import torch

from pprint import pprint

"""# Tokenizacao"""

# Carrega o tokenizer e o modelo pré-treinado do BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Modelo base em inglês (sem distinção entre maiúsculas e minúsculas)
model = BertModel.from_pretrained('bert-base-uncased')

sentenca = "The world has been changing very quickly."

input_ids = tokenizer.encode(sentenca, add_special_tokens=True)
input_ids

# Converte os IDs dos tokens para tensores
input_ids = torch.tensor([input_ids])
input_ids

"""# Obtencao Embedding Bert"""

with torch.no_grad():
    outputs = model(input_ids)

"""A frase (sequência) de entrada contêm 7 palavras e ao ser tokenizada e adicionado tokens especiais que orientam o modelo como (CLS,SEP,PAD ou até mesmo UNK), geram um tensor com 10 palavras devido a adição do tokens especiais que também serão contabilizados."""

pprint(outputs)

"""## last_hidden_state"""

outputs.keys()

"""* last_hidden_state (Último Estado Oculto)

  Formato: É um tensor 3D de tamanho (batch_size, sequence_length, hidden_size).
  
  * batch_size: Número de sequências processadas em lote. no caso abaixo contem apenas 1 batch
  * sequence_length: Comprimento da sequência de entrada (número de tokens), a entrada original possui 7 palavras e ao ser tokenizada passsou a ter 10 tokens que é composo pelo token da palvras + os tokens especiais.
  * hidden_size: Dimensionalidade do embedding (768 para o BERT base).

* Uso: É usado para tarefas que requerem representações de cada palavra no contexto da frase, como:
  * Marcação de sequência (Sequence Tagging): Ex: Part-of-Speech tagging, Named Entity Recognition.
  * Extração de características (Feature Extraction): Usar os embeddings como entrada para outros modelos.
  * Similaridade semântica de palavras: Comparar os embeddings de palavras diferentes.
  * Resposta a perguntas (Question Answering): Identificar a resposta em um contexto.

O tensor retornado outputs['last_hidden_state'].shape é 3d e  o primerio elemento 1 representa o tamanho do batch, o segundo elemento quantidade de palavras na sentença 10 e o terceiro 768 a quantidade de elemento em cada vetor de palavras que correponde ao tipo de modelo utilizado large(1024) ou base(768)
"""

outputs['last_hidden_state'].shape

"""## embedding como um tensor 2d pooler_output"""

# Os embeddings estão em outputs[0]
embeddings = outputs[0]
embeddings

"""* Formato: É um tensor 2D de tamanho (batch_size, hidden_size).
 * batch_size: Número de sequências processadas em lote.
 * hidden_size: Dimensionalidade do embedding (ex: 768 para o BERT base).
 * Conteúdo: É derivado do embedding do token [CLS] presente no last_hidden_state. O embedding do [CLS] passa por uma camada linear (com função de ativação tanh) para gerar o pooler_output.
 * Uso: É usado principalmente para tarefas de classificação de sequências ou para obter uma representação vetorial única da sequência inteira, como:
 * Classificação de texto: Ex: análise de sentimentos, classificação de tópicos.
Inferência de linguagem natural (NLI): Determinar a relação entre duas frases (entailment, contradiction, neutral).

"""

outputs.keys()

embeddings_pooler = outputs[1]
embeddings_pooler

# Imprime o tamanho dos embeddings
print("Tamanho dos embeddings:", embeddings_pooler.shape)

"""# Análises"""

# Imprime o embedding do token [CLS] (primeiro token)
cls_embedding = embeddings[0][0]
print("\nEmbedding do [CLS]:", cls_embedding[:10]) # Imprime os 10 primeiros valores do vetor



# Imprime o embedding do token "cat" (terceiro token)
has_embedding = embeddings[0][2]
print("\nEmbedding de 'has':", has_embedding[:10]) # Imprime os 10 primeiros valores do vetor

# Converte os IDs dos tokens para palavras
tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
print("\nTokens:", tokens)